{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f56ea78b",
   "metadata": {},
   "source": [
    "## Fetch the runs and collate information in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5681470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "api = wandb.Api()\n",
    "entity, project = \"sayakpaul\", \"keras-xla-benchmarks\"  \n",
    "runs = api.runs(entity + \"/\" + project) \n",
    "print(f\"Total runs: {len(runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb85ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_mapping import MODEL_NAME_MAPPING\n",
    "\n",
    "all_variants = [\n",
    "    variant for k in MODEL_NAME_MAPPING for variant in MODEL_NAME_MAPPING[k]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f3f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "resolutions = []\n",
    "accelerators = []\n",
    "\n",
    "model_families = []\n",
    "model_variants = []\n",
    "xla_status = []\n",
    "\n",
    "flops = []\n",
    "params = []\n",
    "throughputs = []\n",
    "\n",
    "for run in runs:\n",
    "    run_config = run.config\n",
    "    run_summary = run.summary._json_dict\n",
    "\n",
    "    if run_config[\"variant\"] in all_variants:\n",
    "        model_families.append(run_config[\"family\"])\n",
    "        model_variants.append(run_config[\"variant\"])\n",
    "        resolutions.append(run_config[\"resolution\"])\n",
    "        xla_status.append(run_config[\"xla\"])\n",
    "\n",
    "        accelerator_name = run.name.split(\"@\")[-1].split(\"-\")[1]\n",
    "        accelerators.append(accelerator_name)\n",
    "\n",
    "        flops.append(run_summary[\"FLOPs (giga)\"])\n",
    "        params.append(run_summary[\"Num parameters (million)\"])\n",
    "        throughputs.append(run_summary[\"Throughput (samples/sec)\"])\n",
    "\n",
    "viz_df = pd.DataFrame(\n",
    "    {\n",
    "        \"model_family\": model_families,\n",
    "        \"model_variant\": model_variants,\n",
    "        \"resolution\": resolutions,\n",
    "        \"xla\": xla_status,\n",
    "        \"accelerator\": accelerators,\n",
    "        \"flop (giga)\": flops,\n",
    "        \"params (million)\": params,\n",
    "        \"throughput (samples/sec)\": throughputs,\n",
    "    }\n",
    ")\n",
    "viz_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7285a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_df[\"accelerator\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35506bb4",
   "metadata": {},
   "source": [
    "## Filter w.r.t accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb3904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topk_per_accelerator(\n",
    "    accelerator=\"a100\", topk=10, resolution=224, xla_status=True\n",
    "):\n",
    "    filtered_df = viz_df[viz_df[\"accelerator\"] == accelerator]\n",
    "    subset_df = filtered_df.query(f\"resolution == {resolution} and xla == {xla_status}\")\n",
    "    topk_df = subset_df.nlargest(topk, [\"throughput (samples/sec)\"])\n",
    "    return topk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679cce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from\n",
    "# https://github.com/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_metrics(df, savefig=False):\n",
    "    for model_variant in df[\"model_variant\"]:\n",
    "        filtered = df.query(f\"model_variant == '{model_variant}'\")\n",
    "        plt.scatter(\n",
    "            filtered[\"flop (giga)\"],\n",
    "            filtered[\"throughput (samples/sec)\"],\n",
    "            alpha=0.5,\n",
    "            s=filtered[\"params (million)\"] * 5,\n",
    "            label=model_variant,\n",
    "            marker=\"o\",\n",
    "        )\n",
    "\n",
    "    legend = plt.legend(bbox_to_anchor=(1, 1))\n",
    "    for handle in legend.legendHandles:\n",
    "        handle.set_sizes([20])\n",
    "\n",
    "    plt.ylabel(\"Throughput (samples/sec)\", fontsize=14)\n",
    "    plt.xlabel(\"FLOPS (giga)\", fontsize=14)\n",
    "\n",
    "    accelerator_name = df[\"accelerator\"].unique()[0]\n",
    "    resolution = df[\"resolution\"].unique()[0]\n",
    "    xla_status = df[\"xla\"].unique()[0]\n",
    "    plt.title(\n",
    "        f\"Accelerator: {accelerator_name}, Resolution: {resolution}, XLA: {xla_status}\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "    if not savefig:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plot_name = f\"{accelerator_name}_{resolution}_{xla_status}.png\"\n",
    "        plt.savefig(plot_name, dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae2409f",
   "metadata": {},
   "source": [
    "### A100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb2c21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a100_df = plot_topk_per_accelerator(\"a100\")\n",
    "plot_metrics(a100_df, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba33e8",
   "metadata": {},
   "source": [
    "### V100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ac8cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "v100_df = plot_topk_per_accelerator(\"v100\")\n",
    "plot_metrics(v100_df, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0495b6",
   "metadata": {},
   "source": [
    "### T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd245f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t4_df = plot_topk_per_accelerator(\"t4\")\n",
    "plot_metrics(t4_df, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2f939",
   "metadata": {},
   "source": [
    "For each accelerator type, the trend of the models leading to the highest amount of throughput seem to vary a little bit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c832d82",
   "metadata": {},
   "source": [
    "## Resolution-wise throughput distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343d6aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_df.resolution.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6b8dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the dataframe by unique resolutions and finding the \n",
    "# model variant with highest throughput per group.\n",
    "grouped = viz_df.groupby(\"resolution\")[\"throughput (samples/sec)\"].idxmax()\n",
    "\n",
    "# Selecting the rows with the highest throughput per group.\n",
    "result = viz_df.loc[grouped, viz_df.columns]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e882d6c",
   "metadata": {},
   "source": [
    "This means that for each resolution the highest throughput seems to have a relationship with the accelerator used for running the benchmark. \n",
    "\n",
    "What happens if we take a group by the accelerator too?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e02d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = viz_df.groupby([\"resolution\", \"accelerator\"])[\n",
    "    \"throughput (samples/sec)\"\n",
    "].idxmax()\n",
    "result = viz_df.loc[grouped, viz_df.columns]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb14f36",
   "metadata": {},
   "source": [
    "## Highest amount of speedup from XLA grouped by model family\n",
    "\n",
    "Thanks to ChatGPT for the code used in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd031f4",
   "metadata": {},
   "source": [
    "### Absolute speedup\n",
    "\n",
    "_Which model family has the highest amount of speedup from XLA for a particular resolution (say 224) and accelerator (say A100)?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea6091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows w.r.t the resolution of 224 and A100 accelerator.\n",
    "viz_df_224_a100 = viz_df.query(f\"resolution == 224 and accelerator == 'a100'\")\n",
    "\n",
    "# Filter rows where xla is True (XLA enabled).\n",
    "xla_enabled = viz_df_224_a100[viz_df_224_a100[\"xla\"]]\n",
    "\n",
    "# Filter rows where xla is False (XLA disabled).\n",
    "xla_disabled = viz_df_224_a100[~viz_df_224_a100[\"xla\"]]\n",
    "\n",
    "# Group by 'model_family' and calculate the speedup for each model variant.\n",
    "grouped = []\n",
    "for model_family, group in xla_enabled.groupby(\"model_family\"):\n",
    "    for model_variant, variant_group in group.groupby(\"model_variant\"):\n",
    "        throughput_with_xla = variant_group[\"throughput (samples/sec)\"].values[0]\n",
    "        throughput_without_xla = xla_disabled[\n",
    "            (xla_disabled[\"model_family\"] == model_family)\n",
    "            & (xla_disabled[\"model_variant\"] == model_variant)\n",
    "        ][\"throughput (samples/sec)\"].values[0]\n",
    "        speedup = throughput_with_xla - throughput_without_xla\n",
    "        grouped.append(\n",
    "            {\n",
    "                \"model_family\": model_family,\n",
    "                \"model_variant\": model_variant,\n",
    "                \"speedup\": speedup,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Create a dataframe from the grouped results.\n",
    "result = pd.DataFrame(grouped)\n",
    "\n",
    "# Find the model variant with the highest speedup per model family.\n",
    "max_speedup = result.groupby(\"model_family\")[\"speedup\"].idxmax()\n",
    "result_max_speedup = result.loc[max_speedup]\n",
    "result_max_speedup.sort_values(by=\"speedup\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c44d30b",
   "metadata": {},
   "source": [
    "### In terms of relative percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ee20a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where xla is True (XLA enabled).\n",
    "xla_enabled = viz_df_224_a100.query(\"xla == True\")\n",
    "\n",
    "# Filter rows where xla is False (XLA disabled).\n",
    "xla_disabled = viz_df_224_a100.query(\"xla == False\")\n",
    "\n",
    "# Group by 'model_family' and calculate the speedup for each model variant.\n",
    "grouped = []\n",
    "for model_family, group in xla_enabled.groupby(\"model_family\"):\n",
    "    for model_variant, variant_group in group.groupby(\"model_variant\"):\n",
    "        throughput_with_xla = variant_group[\"throughput (samples/sec)\"].values[0]\n",
    "        throughput_without_xla = xla_disabled.query(\n",
    "            \"model_family == @model_family and model_variant == @model_variant\"\n",
    "        )[\"throughput (samples/sec)\"].values[0]\n",
    "        speedup_percentage = (\n",
    "            (throughput_with_xla - throughput_without_xla) / throughput_without_xla\n",
    "        ) * 100\n",
    "        grouped.append(\n",
    "            {\n",
    "                \"model_family\": model_family,\n",
    "                \"model_variant\": model_variant,\n",
    "                \"speedup_percentage\": speedup_percentage,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Create a dataframe from the grouped results.\n",
    "result = pd.DataFrame(grouped)\n",
    "\n",
    "# Find the model variant with the highest speedup percentage per model family.\n",
    "max_speedup = result.groupby(\"model_family\")[\"speedup_percentage\"].idxmax()\n",
    "result_max_speedup = result.loc[max_speedup]\n",
    "result_max_speedup.sort_values(by=\"speedup_percentage\", ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
